{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "J9ZhrUeTDsx-"
      },
      "outputs": [],
      "source": [
        "#Necessary Libraries\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from random import randint\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy import array,argmax,array_equal\n",
        "import keras.backend as K\n",
        "from tensorflow.keras import models,Input\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.models import Sequential, Model, load_model\n",
        "from tensorflow.keras.layers import LSTM, Bidirectional, SimpleRNN, GRU,Lambda,Dense, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import matplotlib.ticker as ticker\n",
        "tf.keras.backend.set_floatx('float64')\n",
        "import csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "M6dnebWKpu99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cca4b314-f553-4295-8174-dec64899cad1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-15 15:00:07--  https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 172.253.117.128, 74.125.20.128, 108.177.98.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|172.253.117.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2008340480 (1.9G) [application/x-tar]\n",
            "Saving to: ‘dakshina_dataset_v1.0.tar’\n",
            "\n",
            "dakshina_dataset_v1 100%[===================>]   1.87G   228MB/s    in 7.7s    \n",
            "\n",
            "2022-05-15 15:00:15 (248 MB/s) - ‘dakshina_dataset_v1.0.tar’ saved [2008340480/2008340480]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Loading the dakshina dataset\n",
        "\n",
        "!wget https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\n",
        "!tar -xf dakshina_dataset_v1.0.tar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "DFMo0XNnqDq_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "127e8b3c-42df-4981-cc5b-e0f5ec1211c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hi.translit.sampled.dev.tsv   hi.translit.sampled.train.tsv\n",
            "hi.translit.sampled.test.tsv\n"
          ]
        }
      ],
      "source": [
        "#Selecting the Hindi language\n",
        "\n",
        "!ls dakshina_dataset_v1.0/hi/lexicons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "JkG0hgUAqER_"
      },
      "outputs": [],
      "source": [
        "#Directory for Training,testidation and Testing\n",
        "train_dir = \"./dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\"\n",
        "\n",
        "test_dir = \"./dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "PBzvL2e20Sx-"
      },
      "outputs": [],
      "source": [
        "# Reading the raw corpus\n",
        "#returns the native(Hindi) and romanized(English) versions of the words in the corpus\n",
        "\n",
        "import io\n",
        "def raw_corpus(crp):\n",
        "  Eng = []\n",
        "  Hindi= []\n",
        "  with io.open(crp, encoding ='utf-8') as f:\n",
        "    for line in f:\n",
        "      if '\\t' not in line:\n",
        "        continue\n",
        "      tokens = line.rstrip().split(\"\\t\")\n",
        "      Eng.append(tokens[1])\n",
        "      Hindi.append(tokens[0])\n",
        "  return Eng, Hindi "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "K-VCpVH93sFh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce1aece0-38c0-4b48-cff2-55e4ec6d643c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training examples:  44204\n",
            "Testing examples:  4502\n"
          ]
        }
      ],
      "source": [
        "train_src, train_tgt = raw_corpus(train_dir)\n",
        "\n",
        "test_src, test_tgt = raw_corpus(test_dir)\n",
        "\n",
        "print(\"Training examples: \", len(train_src))\n",
        "print(\"Testing examples: \", len(test_src))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "BvnPFZvV4yKI"
      },
      "outputs": [],
      "source": [
        "ip_txt_ns = []\n",
        "tgt_txt_ns = []\n",
        "test_ip_txt_ns = []\n",
        "test_tgt_txt_ns = []\n",
        "ip_char = set()\n",
        "tgt_char = set()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "_u0pkPoq5GSq"
      },
      "outputs": [],
      "source": [
        "for (txt_ip, txt_tgt) in zip(train_src, train_tgt):\n",
        "    # tab : \"start sequence\" character\n",
        "    # \\n  : \"end sequence\" character\n",
        "    txt_tgt = \"B\" + txt_tgt + \"E\"\n",
        "    ip_txt_ns.append(txt_ip)\n",
        "    tgt_txt_ns.append(txt_tgt)\n",
        "\n",
        "    for char in txt_ip:\n",
        "        if char not in ip_char:\n",
        "            ip_char.add(char)\n",
        "\n",
        "    for char in txt_tgt:\n",
        "        if char not in tgt_char:\n",
        "            tgt_char.add(char)\n",
        "\n",
        "\n",
        "for (txt_ip, txt_tgt) in zip(test_src, test_tgt):\n",
        "    # tab : \"start sequence\" character\n",
        "    # \\n  : \"end sequence\" character\n",
        "    txt_tgt = \"B\" + txt_tgt + \"E\"\n",
        "    test_ip_txt_ns.append(txt_ip)\n",
        "    test_tgt_txt_ns.append(txt_tgt)\n",
        "    for char in txt_ip:\n",
        "        if char not in ip_char:\n",
        "            ip_char.add(char)\n",
        "    for char in txt_tgt:\n",
        "        if char not in tgt_char:\n",
        "            tgt_char.add(char)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "YcpWebDU4o6_"
      },
      "outputs": [],
      "source": [
        "#Shuffling the Training and testidation dataset\n",
        "\n",
        "train_arr = np.arange(len(train_src))\n",
        "np.random.shuffle(train_arr)\n",
        "test_arr = np.arange(len(test_src))\n",
        "np.random.shuffle(test_arr)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "SD6j9o7H5U1g"
      },
      "outputs": [],
      "source": [
        "ips_txt = []\n",
        "tgts_txt = []\n",
        "\n",
        "for i in range(len(train_src)):\n",
        "    ips_txt.append(ip_txt_ns[train_arr[i]])\n",
        "    tgts_txt.append(tgt_txt_ns[train_arr[i]])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_ip_txt = []\n",
        "test_tgt_txt = []\n",
        "\n",
        "for i in range(len(test_src)):\n",
        "    test_ip_txt.append(test_ip_txt_ns[test_arr[i]])\n",
        "    test_tgt_txt.append(test_tgt_txt_ns[test_arr[i]])"
      ],
      "metadata": {
        "id": "5IslLeQE4K80"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "FHadGBBO5oEW"
      },
      "outputs": [],
      "source": [
        "ip_char.add(\" \")\n",
        "tgt_char.add(\" \")\n",
        "ip_char = sorted(list(ip_char))\n",
        "tgt_char = sorted(list(tgt_char))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "enc_tokens = len(ip_char)\n",
        "dec_tokens= len(tgt_char)"
      ],
      "metadata": {
        "id": "iMuAKfUb4W4h"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_enc_seq_length = max([len(txt) for txt in ips_txt])\n",
        "max_dec_seq_length = max([len(txt) for txt in tgts_txt])\n",
        "test_max_enc_seq_length = max([len(txt) for txt in test_ip_txt])\n",
        "test_max_dec_seq_length = max([len(txt) for txt in test_tgt_txt])"
      ],
      "metadata": {
        "id": "-MhTrTZG5Aqr"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "OtHaPQPw6VuP"
      },
      "outputs": [],
      "source": [
        "ip_tk_idx= dict([(j, k) for k, j in enumerate(ip_char)])\n",
        "tgt_tk_idx= dict([(j, k) for k, j in enumerate(tgt_char)])\n",
        "rev_src_char_idx = dict((i, char) for char, i in ip_tk_idx.items())\n",
        "rev_tgt_char_idx = dict((i, char) for char, i in tgt_tk_idx.items())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "CtAojV-O6p0G"
      },
      "outputs": [],
      "source": [
        "trc_ip_txt = ips_txt[:44160]\n",
        "trc_tgt_txt = tgts_txt[:44160]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "31iEPN2D-UPA"
      },
      "outputs": [],
      "source": [
        "ip_encd = np.zeros(\n",
        "    (len(trc_ip_txt), max_enc_seq_length, enc_tokens), dtype=\"float64\"\n",
        ")\n",
        "tgt_decd = np.zeros(\n",
        "    (len(trc_ip_txt), max_dec_seq_length, dec_tokens), dtype=\"float64\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "fCZN_YHH-ZeJ"
      },
      "outputs": [],
      "source": [
        "for i, (txt_ip, txt_tgt) in enumerate(zip(trc_ip_txt, trc_tgt_txt)):\n",
        "    for m, n in enumerate(txt_ip):\n",
        "        ip_encd[i, m, ip_tk_idx[n]] = 1.0\n",
        "    ip_encd[i, m + 1 :, ip_tk_idx[\" \"]] = 1.0\n",
        "    for m, n in enumerate(txt_tgt):\n",
        "        tgt_decd[i, m, tgt_tk_idx[n]] = 1.0\n",
        "    tgt_decd[i, m + 1 :, tgt_tk_idx[\" \"]] = 1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "KWmlTIC8-dTu"
      },
      "outputs": [],
      "source": [
        "test_ip_encd= np.zeros(\n",
        "    (len(test_ip_txt), max_enc_seq_length, enc_tokens), dtype=\"float64\"\n",
        ")\n",
        "test_tgt_decd = np.zeros(\n",
        "    (len(test_tgt_txt), max_dec_seq_length, dec_tokens), dtype=\"float64\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "KSyas_TA-gj1"
      },
      "outputs": [],
      "source": [
        "for i, (txt_ip, txt_tgt) in enumerate(zip(test_ip_txt, test_tgt_txt)):\n",
        "    \n",
        "    for t, n in enumerate(txt_ip):\n",
        "        test_ip_encd[i, t, ip_tk_idx[n]] = 1.0\n",
        "    test_ip_encd[i, t + 1 :, ip_tk_idx[\" \"]] = 1.0\n",
        "\n",
        "    for t, n in enumerate(txt_tgt):\n",
        "        test_tgt_decd[i, t, tgt_tk_idx[n]] = 1.0\n",
        "    test_tgt_decd[i, t + 1: , tgt_tk_idx[\" \"]] = 1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "N-RyyRhTQ2XC"
      },
      "outputs": [],
      "source": [
        "class Bahdanau(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(Bahdanau, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "    \n",
        "  def call(self, query, testue):\n",
        "    \n",
        "    query_with_time_axis = tf.expand_dims(query, 1)\n",
        "    \n",
        "    score = self.V(tf.nn.tanh(self.W1(query_with_time_axis) + self.W2(testue)))\n",
        "    \n",
        "    aw = tf.nn.softmax(score, axis=1)\n",
        "    vc = aw * testue\n",
        "    vc = tf.reduce_sum(vc, axis=1)\n",
        "\n",
        "    return vc, aw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "2mfk2-afoCeE"
      },
      "outputs": [],
      "source": [
        "class Seq_to_Seq_with_attention(object):\n",
        "\n",
        "  def __init__(self, cell = 'RNN', hidden_layer=32, learning_rate= 1e-3, drop_out = 0.3,\n",
        "               epochs = 10, batch_size = 32, attention = 'bahdanau'):\n",
        "    \n",
        "    self.cell = cell\n",
        "    self.hidden_layer = hidden_layer\n",
        "    self.learning_rate = learning_rate\n",
        "    self.drop_out = drop_out\n",
        "    self.epochs = epochs\n",
        "    self.batch_size = batch_size\n",
        "    self.attention = attention\n",
        "\n",
        "  def fit_model(self, ip_encd, tgt_decd):\n",
        "\n",
        "    ip_encds = Input(shape=(max_enc_seq_length, enc_tokens), name='encoder_inputs')\n",
        "\n",
        "    if self.cell == 'LSTM':\n",
        "\n",
        "      enc_lstm = LSTM(self.hidden_layer,return_sequences=True, return_state=True, dropout = self.drop_out, name='encoder_lstm')\n",
        "      enc_ops, enc_hs, enc_cs = enc_lstm(ip_encds)\n",
        "      states_enc = [enc_hs, enc_cs]\n",
        "\n",
        "    elif self.cell == 'RNN':\n",
        "\n",
        "      enc_rnn = SimpleRNN(self.hidden_layer,return_sequences=True, return_state=True, dropout = self.drop_out, name='encoder_rnn')\n",
        "      enc_ops, enc_hs = enc_rnn(ip_encds)\n",
        "      states_enc = [enc_hs]\n",
        "\n",
        "    elif self.cell == 'GRU':\n",
        "\n",
        "      enc_gru = GRU(self.hidden_layer,return_sequences=True, return_state=True, dropout = self.drop_out, name='encoder_gru')\n",
        "      enc_ops, enc_hs = enc_gru(ip_encds)\n",
        "      states_enc = [enc_hs]\n",
        "\n",
        "    \n",
        "\n",
        "    # Attention Layer\n",
        "    if self.attention == 'bahdanau':\n",
        "      attention= Bahdanau(self.hidden_layer)\n",
        "\n",
        "    # Decoder Layers\n",
        "    inps_deco = Input(shape=(1, (dec_tokens + self.hidden_layer)),name='decoder_inputs')\n",
        "\n",
        "    if self.cell == 'LSTM':\n",
        "\n",
        "      dec_lstm = LSTM(self.hidden_layer, dropout = self.drop_out, return_state=True, name='decoder_lstm')\n",
        "    \n",
        "    elif self.cell == 'GRU':\n",
        "\n",
        "      dec_gru = GRU(self.hidden_layer, dropout = self.drop_out, return_state=True, name='decoder_gru')\n",
        "    \n",
        "    elif self.cell == 'RNN':\n",
        "\n",
        "      dec_rnn = SimpleRNN(self.hidden_layer, dropout = self.drop_out, return_state=True, name='decoder_rnn')  \n",
        "    \n",
        "    \n",
        "    dec_dense = Dense(dec_tokens, activation='softmax',  name='decoder_dense')\n",
        "    all_ops = []\n",
        "\n",
        "    ips = np.zeros((self.batch_size, 1, dec_tokens))\n",
        "    ips[:, 0, 0] = 1 \n",
        "\n",
        "    dec_ops = enc_hs\n",
        "    states = states_enc\n",
        "\n",
        "    for _ in range(max_dec_seq_length):\n",
        "\n",
        "      vc, aw = attention(dec_ops, enc_ops)\n",
        "      vc = tf.expand_dims(vc, 1)\n",
        "      \n",
        "      ips = tf.concat([vc, ips], axis=-1)\n",
        "\n",
        "      if self.cell == 'LSTM':\n",
        "\n",
        "        dec_ops, hs, cs = dec_lstm(ips, initial_state=states)\n",
        "\n",
        "      if self.cell == 'GRU':\n",
        "\n",
        "        dec_ops, hs = dec_gru(ips, initial_state=states)\n",
        "\n",
        "      if self.cell == 'RNN':\n",
        "\n",
        "        dec_ops, hs = dec_rnn(ips, initial_state=states)\n",
        "      \n",
        "      ops = dec_dense(dec_ops)\n",
        "      ops = tf.expand_dims(ops, 1)\n",
        "      all_ops.append(ops)\n",
        "      ips = ops\n",
        "      if self.cell == 'LSTM':\n",
        "\n",
        "        states = [hs, cs]\n",
        "\n",
        "      if self.cell == 'GRU' or self.cell == 'RNN':\n",
        "        \n",
        "        states = [hs]\n",
        "\n",
        "\n",
        "    dec_ops = Lambda(lambda x: K.concatenate(x, axis=1))(all_ops)\n",
        "    model = Model(ip_encds, dec_ops, name='model_encoder_decoder')\n",
        "    \n",
        "    optimizer = Adam(lr=self.learning_rate, beta_1=0.9, beta_2=0.999)\n",
        "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    model.fit(ip_encd, tgt_decd,\n",
        "              batch_size=self.batch_size, \n",
        "              epochs=self.epochs,\n",
        "              #callbacks = [WandbCallback()]\n",
        "              )\n",
        "\n",
        "    pred = model.predict(test_ip_encd[:4352], batch_size=self.batch_size)\n",
        "    dl=['Sno','Input Data','Target data','Predicted Data']\n",
        "    total = 0\n",
        "    right = 0\n",
        "    v_t = 4352\n",
        "\n",
        "    for k in range(v_t):\n",
        "      \n",
        "      ohv = pred[k]\n",
        "      ohv1 = test_tgt_decd[k]\n",
        "      id2 = tf.argmax(ohv, axis=1)\n",
        "      id1 = tf.argmax(ohv1, axis=1)\n",
        "      \n",
        "      if (id2.numpy() == id1.numpy()).all():\n",
        "        right = right + 1\n",
        "        \n",
        "      total = total + 1\n",
        "      accuracy_epoch = right/total\n",
        "      arr= id2.numpy()\n",
        "      dec_seq=''\n",
        "      for i in range(1,len(arr)):\n",
        "            if arr[i] != 2:\n",
        "                dec_seq = dec_seq + rev_tgt_char_idx[arr[i]]\n",
        "\n",
        "      tw = test_tgt_txt[k] \n",
        "      tw = tw[1:len(tw)-1]\n",
        "      dl1 = [k+1, test_ip_txt[k], tw, dec_seq]\n",
        "      dl.append(dl1)\n",
        "\n",
        "    with open('Attention_Predictions.tsv', 'w', newline='', encoding=\"utf-8\") as file:\n",
        "        writer = csv.writer(file, delimiter='\\t')\n",
        "        writer.writerows(dl)\n",
        "\n",
        "    \n",
        "    test_accuracy = right/total\n",
        "    print(test_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#best hyperparameters\n",
        "best_attention = 'bahdanau'\n",
        "best_batch_size = 64\n",
        "best_cell = 'LSTM'\n",
        "best_drop_out = 0.2\n",
        "best_epochs = 15\n",
        "best_hidden_layer = 128\n",
        "best_learning_rate = 0.001"
      ],
      "metadata": {
        "id": "vHLSlKMFWxnF"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnn_model = Seq_to_Seq_with_attention(best_cell, hidden_layer=best_hidden_layer,\n",
        "                learning_rate= best_learning_rate, drop_out=best_drop_out, epochs = best_epochs,\n",
        "                batch_size = best_batch_size)\n",
        "  \n",
        "rnn_model.fit_model(ip_encd,tgt_decd)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtzliTSmW1k7",
        "outputId": "73a87541-5b68-4bc7-8a46-c54907728e07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "591/690 [========================>.....] - ETA: 6s - loss: 1.2436 - accuracy: 0.6890"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Testing_Seq_to_Seq_with_attention.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}